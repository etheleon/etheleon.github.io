---
layout: post
title: "Spark Joy - Saying Konmari to your event logs with grammar of data manipulation"
excerpt: "Using Spark"
categories: articles
tags: [tidyverse, R, spark, sparkly]
author: wesley_goi
comments: true
share: true
modified: 2019-02-20
mathjax: true
image:
  feature: sparklyr_joy_png.png
  thumb: sparklyr_joy_png.png
  credit: Wesley GOI
  creditlink: http://etheleon.github.io
---

# Spark Joy - Saying Konmari to your event logs with grammar of data manipulation


![](https://raw.githubusercontent.com/etheleon/etheleon.github.io/master/images/sparklyr_joy.png)

When you have a tonne of event logs to parse what should the go to *weapon* of choice be? In this article Iâ€™ll share with you our experience with using spark/sparklyr to tackle this. 

At Honestbee ðŸ, our event logs are stored in AWS S3, delivered to us by [Segment](https://segment.com/blog/exactly-once-delivery/),  at 40 minute intervals. The Data(Science) team uses these logs to evaluate the performance of our machine learning models as well as compare their performance, canonical A|B testing. 

In addition, we also use the same logs to track business KPIs like **C**lick **T**hrough **R**ate, **C**onversion **R**ate and GMV.

In this article, I will share how we leverage high memory clusters running Spark to parse the results logs generated from the Food Recommender System.

![](https://raw.githubusercontent.com/etheleon/etheleon.github.io/master/images/recommender.png)

**Fig:** Whenever an Honestbee customer proceeds to checkout, our ML models will try their best at making personalised prediction for which items youâ€™ll most likely add to cart. Especially things which you missed or . 

> A *post mortem*,  will require us to look through event logs to see which treatment group, based on a weighted distribution,  a user has been assigned to.   

Now, LETS DIVE IN!

Lets begin by importing the necessary libraries

<script src="https://gist.github.com/etheleon/581eeeefed17530f60caa53262232a84.js"></script>

# Connecting with the high memory spark cluster
Next, weâ€™ll need to connect with the Spark master node. 

### Local Cluster

Normally if you're connecting to a locally installed spark cluster you'll set master as `local`. 

Luckily `sparkly` already comes with an inbuilt function to install spark on your local machine:

```r
sparklyr::spark_install(
    version = "2.4.0",
    hadoop_version = "2.7"
)
```

> We are installing Hadoop together with spark, because the module required to read files from the S3 Filesystem with Hadoop  

Next youâ€™ll connect with the cluster and establish a spark connection, sc.

<script src="https://gist.github.com/etheleon/673e551a5573358038896b6dada50721.js"></script>

> **Caution:** At honestbee we do not have a local cluster, so the closest we got is a LARGE EC2 instance which sometimes gives out and you probably want a *managed* cluster set up by DEs or a 3rd party vendor who knows how to deal with cluster management.   

### Remote Clusters

*Alternatively*, thereâ€™s also the option of connecting with a remote cluster via a REST API ie. the R process is not running on the master node but on a remote machine. Often these are managed by 3rd party vendors. At Honestbee, we also chosen this option and the clusters are provisioned by  [Qubole](https://www.qubole.com/) under our AWS account. PS. Pretty good deal!

<script src="https://gist.github.com/etheleon/2d61d1f5a83d1026b5f3dfa9eaa989b3.js"></script>

The gist above sets up a spark connection `sc`, you will need to use this object in most of the functions. 

Separately, because we are reading from S3, we will have to set the S3 access keys and secret. This has to be set before executing functions like `spark_read_json`

<script src="https://gist.github.com/etheleon/f05cc79ad5cd6dc0eb3dbfc2e1bbedcc.js"></script>

> So you would ask what are the pros and cons of each. Local clusters generally are good for EDA since you will be communicating through a REST API (LIVY).   
>   
# Reading JSON logs 
There are essentially two ways to read logs. The first is to read them in as a whole chunks or as a stream â€” as they get dumped into your bucket. 

Thereâ€™s two functions, `spark_read_json` and `stream_read_json` the former is batched and the later creates a structured data stream. Thereâ€™s also the equivalent of for reading your Parquet files 

## Batched
The path should be set with the `s3a` protocol. 
`s3a://segment_bucket/segment-logs/<source_id>/1550361600000`

```r
json_input = spark_read_json(
    sc = sc, 
    name= "logs",
    path= s3, 
    overwrite=T)
```

Belowâ€™s where the magic begins:

<script src="https://gist.github.com/etheleon/2a513d1000c38020a3a834a34e6d5e03.js"></script>

As you can see itâ€™s a simple query, 

1. Filter for all `Added to Cart` events from the `Food` vertical
2. Select following columns:
    * `CartID` 
    * `experiment_id`
    * `variant` (treatment_group) and 
    * `timestamp`
* Remove events where users were not assigned to a model
* Add new columns
    * `fulltime` readable time
    * `time` the hour of the day
* Group the logs by service `recommender` and count the number of rows 
* Add a new column `event` with the value `Added to Cart`
* Sort by time

## Spark Streams
Alternatively, you could also write the results of the above manipulation to a structured spark stream. 

<script src="https://gist.github.com/etheleon/bf72bee8d790cf4f16d76cbb233f7a9d.js"></script>

You can preview these the results from the stream using the `tbl` function coupled to `glimpse`. 

```
sc %>% 
    tbl("data_stream") %>% 
    glimpse
```

```
Observations: ??
Variables: 2
Database: spark_connection
$ expt <chr> "Model_A", "Model_B"
$ n    <dbl> 5345, 621
```

And thatâ€™s it folks on using Sparklyr with your event logs. 

## Model Metadata

![](https://raw.githubusercontent.com/etheleon/etheleon.github.io/master/images/model_graph.png)

With that many models in the wild, itâ€™s hard to keep track of whatâ€™s going on. For my PhD, I personally worked on using Graph Databases to store data with complex relationships and we are currently working on coming up with such a system to store metadata related to our models. 

For example 
1. Which APIs they are associated with
2. What airflow / argo jobs are these models being retrained with 
3. What helm-charts and deployments metadata these models have 
4. And of course meta data like the performance and scores. 

Come talk to us, we are hiring! [Data Engineer](https://boards.greenhouse.io/honestbee/jobs/1426737), [Senior Data Scientist](https://boards.greenhouse.io/honestbee/jobs/1427566)
